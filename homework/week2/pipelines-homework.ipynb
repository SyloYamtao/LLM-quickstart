{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4698c3c-70de-4ead-8594-e26aba1662a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU device name: NVIDIA GeForce RTX 4090\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "\n",
    "# 检查是否可以访问 CUDA\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08887b87-077a-48db-9bfa-f7c24db921a6",
   "metadata": {},
   "source": [
    "### xlm-roberta-base-language-detection\n",
    "This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.\n",
    "\n",
    "#### Model description\n",
    "This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.\n",
    "\n",
    "#### Intended uses & limitations\n",
    "You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages:\n",
    "\n",
    "arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\n",
    "\n",
    "#### Model page\n",
    "https://huggingface.co/papluca/xlm-roberta-base-language-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9684ec5f-9460-4876-9883-69380eacb0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'zh', 'score': 0.9885185360908508}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"sentiment-analysis\",model = model_path + \"xlm-roberta-base-language-detection\")\n",
    "pipe(\"今儿上海可真冷啊\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a12486-952a-447d-9e90-6b041349a26e",
   "metadata": {},
   "source": [
    "### 测试更多示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee67f529-3d32-4834-b29a-e51a65cf4d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'zh', 'score': 0.9933799505233765}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"我觉得这家店蒜泥白肉的味道一般\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e104034-94ca-4370-8d35-438501c29ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'zh', 'score': 0.9934589266777039}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"你学东西真的好快，理论课一讲就明白了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3d11aa-d523-491c-8910-bdc8aba49487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'en', 'score': 0.9935345649719238}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"You learn things really quickly. You understand the theory class as soon as it is taught.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09df7300-2f0d-4bc4-9572-0631658e8253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'en', 'score': 0.962114691734314}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Today Shanghai is really cold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dbfc3-5347-4b82-8ea1-4e6c3d81c07f",
   "metadata": {},
   "source": [
    "### 批处理调用模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1390396e-1af8-497f-85d9-6c9e984b4609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'en', 'score': 0.962114691734314},\n",
       " {'label': 'en', 'score': 0.9939622282981873},\n",
       " {'label': 'en', 'score': 0.9935345649719238}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Today Shanghai is really cold.\",\n",
    "    \"I think the taste of the garlic mashed pork in this store is average.\",\n",
    "    \"You learn things really quickly. You understand the theory class as soon as it is taught.\"\n",
    "]\n",
    "\n",
    "pipe(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a27fe-87d0-45fc-a31a-9a8db23e290a",
   "metadata": {},
   "source": [
    "## 使用 Pipeline API 调用更多预定义任务\n",
    "\n",
    "## Natural Language Processing(NLP)\n",
    "\n",
    "**NLP**(自然语言处理)任务是最常见的任务类型之一，因为文本是我们进行交流的一种自然方式。要将文本转换为模型可识别的格式，需要对其进行分词。这意味着将一系列文本划分为单独的单词或子词（标记），然后将这些标记转换为数字。结果就是，您可以将一系列文本表示为一系列数字，并且一旦您拥有了一系列数字，它就可以输入到模型中来解决各种NLP任务！\n",
    "\n",
    "上面演示的 文本分类任务，以及接下来的标记、问答等任务都属于 NLP 范畴。\n",
    "\n",
    "### Token Classification\n",
    "\n",
    "在任何NLP任务中，文本都经过预处理，将文本序列分成单个单词或子词。这些被称为tokens。\n",
    "\n",
    "**Token Classification**（Token分类）将每个token分配一个来自预定义类别集的标签。\n",
    "\n",
    "两种常见的 Token 分类是：\n",
    "\n",
    "- 命名实体识别（NER）：根据实体类别（如组织、人员、位置或日期）对token进行标记。NER在生物医学设置中特别受欢迎，可以标记基因、蛋白质和药物名称。\n",
    "- 词性标注（POS）：根据其词性（如名词、动词或形容词）对标记进行标记。POS对于帮助翻译系统了解两个相同的单词如何在语法上不同很有用（作为名词的银行与作为动词的银行）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0102a-df4d-4164-8679-35c4dba13600",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tner/roberta-large-ontonotes5\n",
    "This model is a fine-tuned version of roberta-large on the tner/ontonotes5 dataset. Model fine-tuning is done via T-NER's hyper-parameter search (see the repository for more detail). It achieves the following results on the test set:\n",
    "\n",
    "F1 (micro): 0.908632361399938\n",
    "Precision (micro): 0.905148095909732\n",
    "Recall (micro): 0.9121435551212579\n",
    "F1 (macro): 0.8265477704565624\n",
    "Precision (macro): 0.8170668848546687\n",
    "Recall (macro): 0.8387672780349001\n",
    "\n",
    "#### Model page\n",
    "https://huggingface.co/tner/roberta-large-ontonotes5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1ed125-f9ed-42d9-b102-dbc3172baefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"ner\",model = model_path + \"roberta-large-ontonotes5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bab77c-0fe6-4781-b978-02d56829db33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ORG', 'score': 0.9999, 'index': 1, 'word': 'Hug', 'start': 0, 'end': 3}\n",
      "{'entity': 'I-ORG', 'score': 1.0, 'index': 2, 'word': 'ging', 'start': 3, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 1.0, 'index': 3, 'word': 'ĠFace', 'start': 8, 'end': 12}\n",
      "{'entity': 'B-NORP', 'score': 0.9999, 'index': 6, 'word': 'ĠFrench', 'start': 18, 'end': 24}\n",
      "{'entity': 'B-GPE', 'score': 0.9994, 'index': 10, 'word': 'ĠNew', 'start': 42, 'end': 45}\n",
      "{'entity': 'I-GPE', 'score': 0.9996, 'index': 11, 'word': 'ĠYork', 'start': 46, 'end': 50}\n",
      "{'entity': 'I-GPE', 'score': 0.9994, 'index': 12, 'word': 'ĠCity', 'start': 51, 'end': 55}\n"
     ]
    }
   ],
   "source": [
    "preds = classifier(\"Hugging Face is a French company based in New York City.\")\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c72d55-e574-444f-96bc-62704022a148",
   "metadata": {},
   "source": [
    "#### 合并实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b441191-6156-44b8-a323-db461ad06efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9999604,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 0,\n",
       "  'end': 12},\n",
       " {'entity_group': 'NORP',\n",
       "  'score': 0.99992514,\n",
       "  'word': ' French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'GPE',\n",
       "  'score': 0.9994443,\n",
       "  'word': ' New York City',\n",
       "  'start': 42,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"ner\",model =  model_path +\"roberta-large-ontonotes5\", aggregation_strategy=\"simple\")\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dda97-bd1a-4cb0-9636-47c2308c6289",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "**Question Answering**(问答)是另一个token-level的任务，返回一个问题的答案，有时带有上下文（开放领域），有时不带上下文（封闭领域）。每当我们向虚拟助手提出问题时，例如询问一家餐厅是否营业，就会发生这种情况。它还可以提供客户或技术支持，并帮助搜索引擎检索您要求的相关信息。\n",
    "\n",
    "有两种常见的问答类型：\n",
    "\n",
    "- 提取式：给定一个问题和一些上下文，模型必须从上下文中提取出一段文字作为答案\n",
    "- 生成式：给定一个问题和一些上下文，答案是根据上下文生成的；这种方法由`Text2TextGenerationPipeline`处理，而不是下面展示的`QuestionAnsweringPipeline`\n",
    "\n",
    "This model can be used for Extractive QA\n",
    "It has been finetuned for 3 epochs on SQuAD2.0.\n",
    "\n",
    "#### Model page\n",
    "https://huggingface.co/timpal0l/mdeberta-v3-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5281b0b-6b57-4884-92e1-cc2677987360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(task=\"question-answering\",model = model_path + \"mdeberta-v3-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca674a51-30a4-4dea-a443-e428925e990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9839, start: 29, end: 54, answer:  huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "preds = question_answerer(\n",
    "    question=\"What is the name of the repository?\",\n",
    "    context=\"The name of the repository is huggingface/transformers\",\n",
    ")\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2158feb-a528-410a-aabf-f3bd7f2726c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9296, start: 114, end: 123, answer:  Beijing.\n"
     ]
    }
   ],
   "source": [
    "preds = question_answerer(\n",
    "    question=\"What is the capital of China?\",\n",
    "    context=\"On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing.\",\n",
    ")\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995cb85-ab8f-4b28-9413-1a83fa3e4c4d",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "**Summarization**(文本摘要）从较长的文本中创建一个较短的版本，同时尽可能保留原始文档的大部分含义。摘要是一个序列到序列的任务；它输出比输入更短的文本序列。有许多长篇文档可以进行摘要，以帮助读者快速了解主要要点。法案、法律和财务文件、专利和科学论文等文档可以摘要，以节省读者的时间并作为阅读辅助工具。\n",
    "\n",
    "与问答类似，摘要有两种类型：\n",
    "\n",
    "- 提取式：从原始文本中识别和提取最重要的句子\n",
    "- 生成式：从原始文本中生成目标摘要（可能包括输入文件中没有的新单词）；`SummarizationPipeline`使用生成式方法\n",
    "\n",
    "T5 model for multilingual text Summary in English, Russian and Chinese language\n",
    "This model is designed to perform the task of controlled generation of summary text content in multitasking mode with a built-in translation function for languages: Russian, Chinese, English.\n",
    "\n",
    "This is the T5 multitasking model. Which has a conditionally controlled ability to generate summary text content, and translate this. In total, she understands 12 commands, according to the set prefix:\n",
    "\n",
    "\"summary: \" - to generate simple concise content in the source language\n",
    "\"summary brief: \" - to generate a shortened summary content in the source language\n",
    "\"summary big: \" - to generate elongated summary content in the source language\n",
    "The model can understand text in any language from the list: Russian, Chinese or English. It can also translate the result into any language from the list: Russian, Chinese or English.\n",
    "\n",
    "For translation into the target language, the target language identifier is specified as a prefix \"... to :\". Where lang can take the values: ru, en, zh. The source language may not be specified, in addition, the source text may be multilingual.\n",
    "\n",
    "#### Model page\n",
    "https://huggingface.co/utrobinmv/t5_summary_en_ru_zh_base_2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e028f36-2b50-4ae3-8dce-2f0ac685e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=model_path+\"t5_summary_en_ru_zh_base_2048\",\n",
    "                      min_length=8,\n",
    "                      max_length=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c760bbf-2ae4-4f84-bd5e-32c30ee6bd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"In this week's Scrubbing Up, WMT 2014 English-to-German and World Trade Organization (WMT) \"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    \"\"\"\n",
    "    In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \n",
    "    replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "    For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "    On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "    In the former task our best model outperforms even all previously reported ensembles.\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b50ff52b-e61e-40cd-ab66-a591dc0c3b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Understand the role of transformer-based LLMs. Know the difference between transformers and recurrent neural networks.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    '''\n",
    "    Large language models (LLM) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "    The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "    The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "    Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "    It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "    Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "    This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "    '''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7736791b-9585-4534-9efe-3fae3b7b6ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '姜维在军事上很有见解,既有胆色、明义理,深解兵法意理。此人心存汉室'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    '''\n",
    "    姜维（202年—264年3月3日）[1]，字伯约，凉州天水郡冀县（今甘肃省天水市甘谷县）人。三国时期蜀汉著名军事家。原为曹魏天水郡中郎将，后降蜀汉，深受诸葛亮器重。蒋琬、费袆先后逝世后，姜维总领蜀汉军权，并先后十一次伐魏。其后，司马昭灭蜀汉，姜维在剑阁防守锺会。邓艾出奇兵从阴平小路历经艰辛，突然出现在成都附近，诸葛亮子诸葛瞻战死，后主刘禅降魏，蜀汉灭亡[2]:9。姜维打算利用锺会的野心复国，遂降于锺会并共同发动叛乱，但因事败死于乱军之中，享年六十二岁。\n",
    "生平\n",
    "凉州异才\n",
    "良田百顷，不在一亩；但有远志，不在当归。\n",
    "“”\n",
    "姜维与母亲书, 《三国志·姜维传》注引《杂记》\n",
    "姜维出生于202年，父亲姜冏是天水郡守的佐官，曾任郡功曹，早年于羌、戎叛乱中，战死沙场。姜维与母亲相依为命，喜欢汉朝学者郑玄学说。时常结交一些豪杰，暗中养了些死士，心中有大志。初为曹魏中郎[3]，参天水郡军事[4]:1062。建兴六年（228年）诸葛亮出兵祁山，姜维及功曹梁绪、主簿尹赏、主记梁虔等正与天水太守马遵、雍州刺史郭淮外出巡视，马遵听到蜀军将至且诸县响应，怀疑其麾下的姜维等人皆有异心，遂趁夜逃到上邽。当姜维等人察觉马遵已逃走，想回去，但城门已关闭。去冀城，也被拒门外，遂投降诸葛亮，姜维母亲则滞留魏国[5]。\n",
    "颐和园长廊上的“收姜维”情节\n",
    "诸葛亮征辟姜维为仓曹掾，加奉义将军，封当阳亭侯，时年27岁。诸葛亮曾与张裔、蒋琬书称：“姜维忠勤时事，思虑精密，考察他所拥有之才能，李邵、马良都比不上。此人，乃凉州之上等人才。”(有一说为凉州最杰出之人）又说：“姜维在军事上很有见解，既有胆色、明义理，深解兵法意理。此人心存汉室，才能兼备于人，须先教他操练中虎步兵五六千人，将军事全教给他，当带他进宫，觐见天子。”后来，姜维迁为中监军、征西将军。\n",
    "军旅生涯\n",
    "234年，诸葛亮死于五丈原后，姜维返回成都，为右监军、辅汉将军，统率诸军，进封平襄县侯。238年，随大将军蒋琬（诸葛亮后继者）迁往汉中。蒋琬不久升为大司马，便以姜维为司马，数次率偏军西入。243年，升为镇西大将军，领凉州刺史。247年，升卫将军，与大将军费祎共同行使尚书事权。是年，汶山平康蛮人反叛，姜维率众讨伐平定。出在陇西、南安、金城郡边界，与魏国前将军郭淮、右将军夏侯霸等于洮西交战。249年，刘禅授姜维假节，姜维出兵西平。姜维每次想大举出兵，费祎常不依从，限制给他不超过一万名士兵，因此没有重大斩获[4]:1064。\n",
    "北伐中原\n",
    "主条目：姜维北伐\n",
    "\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72315144-7fae-4848-af79-a70e428b2416",
   "metadata": {},
   "source": [
    "\n",
    "## Audio 音频处理任务\n",
    "\n",
    "音频和语音处理任务与其他模态略有不同，主要是因为音频作为输入是一个连续的信号。与文本不同，原始音频波形不能像句子可以被划分为单词那样被整齐地分割成离散的块。为了解决这个问题，通常在固定的时间间隔内对原始音频信号进行采样。如果在每个时间间隔内采样更多样本，采样率就会更高，音频更接近原始音频源。\n",
    "\n",
    "以前的方法是预处理音频以从中提取有用的特征。现在更常见的做法是直接将原始音频波形输入到特征编码器中，以提取音频表示。这样可以简化预处理步骤，并允许模型学习最重要的特征。\n",
    "\n",
    "### Audio classification\n",
    "\n",
    "**Audio classification**(音频分类)是一项将音频数据从预定义的类别集合中进行标记的任务。这是一个广泛的类别，具有许多具体的应用，其中一些包括：\n",
    "\n",
    "- 声学场景分类：使用场景标签（“办公室”、“海滩”、“体育场”）对音频进行标记。\n",
    "- 声学事件检测：使用声音事件标签（“汽车喇叭声”、“鲸鱼叫声”、“玻璃破碎声”）对音频进行标记。\n",
    "- 标记：对包含多种声音的音频进行标记（鸟鸣、会议中的说话人识别）。\n",
    "- 音乐分类：使用流派标签（“金属”、“嘻哈”、“乡村”）对音乐进行标记。\n",
    "\n",
    "Audio Spectrogram Transformer (fine-tuned on AudioSet)\n",
    "Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository.\n",
    "\n",
    "Disclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\n",
    "\n",
    "Model description\n",
    "The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\n",
    "\n",
    "Usage\n",
    "You can use the raw model for classifying audio into one of the AudioSet classes. See the documentation for more info.\n",
    "\n",
    "#### Model page\n",
    "https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593\n",
    "\n",
    "数据集主页：https://huggingface.co/datasets/superb#er\n",
    "\n",
    "```\n",
    "情感识别（ER）为每个话语预测一个情感类别。我们采用了最广泛使用的ER数据集IEMOCAP，并遵循传统的评估协议：我们删除不平衡的情感类别，只保留最后四个具有相似数量数据点的类别，并在标准分割的五折交叉验证上进行评估。评估指标是准确率（ACC）。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4c1e9-8a67-4c72-8dce-ab326e0bc3b6",
   "metadata": {},
   "source": [
    "#### 前置依赖包安装\n",
    "\n",
    "建议在命令行安装必要的音频数据处理包: ffmpeg\n",
    "\n",
    "```shell\n",
    "$apt update & apt upgrade\n",
    "$apt install -y ffmpeg\n",
    "$pip install ffmpeg ffmpeg-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e1f8a8d-75eb-49ab-9353-6c9d1f384cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"audio-classification\", model=model_path+\"ast-finetuned-audioset-10-10-0.4593\",num_mel_filters=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "204a4159-d14d-4623-8340-93d15abcc549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4208, 'label': 'Speech'},\n",
       " {'score': 0.1793, 'label': 'Rain on surface'},\n",
       " {'score': 0.1301, 'label': 'Rain'},\n",
       " {'score': 0.096, 'label': 'Raindrop'},\n",
       " {'score': 0.0578, 'label': 'Music'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Hugging Face Datasets 上的测试文件\n",
    "preds = classifier(\"https://hf-mirror.com/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2528bf81-8289-4bb9-bf2d-c0ce9f7e8b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4208, 'label': 'Speech'},\n",
       " {'score': 0.1793, 'label': 'Rain on surface'},\n",
       " {'score': 0.1301, 'label': 'Rain'},\n",
       " {'score': 0.096, 'label': 'Raindrop'},\n",
       " {'score': 0.0578, 'label': 'Music'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用本地的音频文件做测试\n",
    "preds = classifier(\"data/audio/mlk.flac\")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073cd1f-ba16-40a7-a029-bbd5e2a53dee",
   "metadata": {},
   "source": [
    "### Automatic speech recognition（ASR）\n",
    "\n",
    "**Automatic speech recognition**（自动语音识别）将语音转录为文本。这是最常见的音频任务之一，部分原因是因为语音是人类交流的自然形式。如今，ASR系统嵌入在智能技术产品中，如扬声器、电话和汽车。我们可以要求虚拟助手播放音乐、设置提醒和告诉我们天气。\n",
    "\n",
    "但是，Transformer架构帮助解决的一个关键挑战是低资源语言。通过在大量语音数据上进行预训练，仅在一个低资源语言的一小时标记语音数据上进行微调，仍然可以产生与以前在100倍更多标记数据上训练的ASR系统相比高质量的结果。\n",
    "### openai/whisper-base\n",
    "Whisper\n",
    "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.\n",
    "\n",
    "Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al from OpenAI. The original code repository can be found here.\n",
    "\n",
    "Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were copied and pasted from the original model card.\n",
    "\n",
    "Model details\n",
    "Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.\n",
    "\n",
    "The models were trained on either English-only data or multilingual data. The English-only models were trained on the task of speech recognition. The multilingual models were trained on both speech recognition and speech translation. For speech recognition, the model predicts transcriptions in the same language as the audio. For speech translation, the model predicts transcriptions to a different language to the audio.\n",
    "\n",
    "#### Model page\n",
    "https://huggingface.co/openai/whisper-base\n",
    "\n",
    "下面展示使用 `OpenAI Whisper Base` 模型实现 ASR 的 Pipeline API 示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d14f8c-1571-4889-abd4-8fde09dc610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 使用 `model` 参数指定模型\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=model_path+\"whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316ebe18-9c8e-4ded-96b7-751304aa9122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = transcriber(\"data/audio/mlk.flac\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b1482-1e13-4557-8d37-5d56e961b5c5",
   "metadata": {},
   "source": [
    "## Computer Vision 计算机视觉\n",
    "\n",
    "**Computer Vision**（计算机视觉）任务中最早成功之一是使用卷积神经网络（CNN）识别邮政编码数字图像。图像由像素组成，每个像素都有一个数值。这使得将图像表示为像素值矩阵变得容易。每个像素值组合描述了图像的颜色。\n",
    "\n",
    "计算机视觉任务可以通过以下两种通用方式解决：\n",
    "\n",
    "- 使用卷积来学习图像的层次特征，从低级特征到高级抽象特征。\n",
    "- 将图像分成块，并使用Transformer逐步学习每个图像块如何相互关联以形成图像。与CNN偏好的自底向上方法不同，这种方法有点像从一个模糊的图像开始，然后逐渐将其聚焦清晰。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bf9f4-6413-409b-968e-f03ca0c88367",
   "metadata": {},
   "source": [
    "### Image Classificaiton\n",
    "\n",
    "**Image Classificaiton**(图像分类)将整个图像从预定义的类别集合中进行标记。像大多数分类任务一样，图像分类有许多实际用例，其中一些包括：\n",
    "\n",
    "- 医疗保健：标记医学图像以检测疾病或监测患者健康状况\n",
    "- 环境：标记卫星图像以监测森林砍伐、提供野外管理信息或检测野火\n",
    "- 农业：标记农作物图像以监测植物健康或用于土地使用监测的卫星图像\n",
    "- 生态学：标记动物或植物物种的图像以监测野生动物种群或跟踪濒危物种\n",
    "\n",
    "### google/vit-base-patch16-224\n",
    "#### Vision Transformer (base-sized model)\n",
    "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\n",
    "\n",
    "Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n",
    "\n",
    "#### Model description\n",
    "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n",
    "\n",
    "Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n",
    "\n",
    "By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n",
    "#### Model page：\n",
    "https://huggingface.co/google/vit-base-patch16-224\n",
    "\n",
    "\n",
    "### google/vit-base-patch16-384\n",
    "#### Vision Transformer (base-sized model)\n",
    "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\n",
    "\n",
    "Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n",
    "\n",
    "#### Model description\n",
    "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at a higher resolution of 384x384.\n",
    "\n",
    "Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n",
    "\n",
    "By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n",
    "\n",
    "#### Intended uses & limitations\n",
    "You can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ca9736-10b9-45b0-a3e3-925f153bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"image-classification\",model=model_path+\"vit-base-patch16-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e14b2211-ac82-409e-8562-7b9ad8877e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2729, 'label': 'lynx, catamount'}\n",
      "{'score': 0.0445, 'label': 'Egyptian cat'}\n",
      "{'score': 0.0424, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.0417, 'label': 'snow leopard, ounce, Panthera uncia'}\n",
      "{'score': 0.0345, 'label': 'tiger cat'}\n"
     ]
    }
   ],
   "source": [
    "preds = classifier(\n",
    "    \"https://hf-mirror.com/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ecb60-ebb9-4f4e-82bc-21e79bb22d90",
   "metadata": {},
   "source": [
    "![](data/image/cat-chonk.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67592129-6430-4349-8016-7dde511ff69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2729, 'label': 'lynx, catamount'}\n",
      "{'score': 0.0445, 'label': 'Egyptian cat'}\n",
      "{'score': 0.0424, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.0417, 'label': 'snow leopard, ounce, Panthera uncia'}\n",
      "{'score': 0.0345, 'label': 'tiger cat'}\n"
     ]
    }
   ],
   "source": [
    "# 使用本地图片（狼猫）\n",
    "preds = classifier(\n",
    "    \"data/image/cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113c251-a648-4ea2-baa4-3081bb490c70",
   "metadata": {},
   "source": [
    "![](data/image/panda.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e9b51a-d759-4398-9894-f10933dbed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9936, 'label': 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca'}\n",
      "{'score': 0.0015, 'label': 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens'}\n",
      "{'score': 0.0005, 'label': 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus'}\n",
      "{'score': 0.0004, 'label': 'sloth bear, Melursus ursinus, Ursus ursinus'}\n",
      "{'score': 0.0002, 'label': 'American black bear, black bear, Ursus americanus, Euarctos americanus'}\n"
     ]
    }
   ],
   "source": [
    "# 使用本地图片（熊猫）\n",
    "preds = classifier(\n",
    "    \"data/image/panda.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118a561-3dc9-4693-8c3b-3e29c37b4d71",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "\n",
    "与图像分类不同，目标检测在图像中识别多个对象以及这些对象在图像中的位置（由边界框定义）。目标检测的一些示例应用包括：\n",
    "\n",
    "- 自动驾驶车辆：检测日常交通对象，如其他车辆、行人和红绿灯\n",
    "- 遥感：灾害监测、城市规划和天气预报\n",
    "- 缺陷检测：检测建筑物中的裂缝或结构损坏，以及制造业产品缺陷\n",
    "\n",
    "模型主页：https://huggingface.co/facebook/detr-resnet-50\n",
    "\n",
    "### facebook/detr-resnet-101\n",
    "#### DETR (End-to-End Object Detection) model with ResNet-101 backbone\n",
    "DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n",
    "\n",
    "Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\n",
    "\n",
    "#### Model description\n",
    "The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.\n",
    "\n",
    "The model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n",
    "#### Model page：\n",
    "https://huggingface.co/facebook/detr-resnet-101\n",
    "\n",
    "### hustvl/yolos-tiny\n",
    "#### YOLOS (tiny-sized) model\n",
    "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository.\n",
    "\n",
    "Disclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.\n",
    "\n",
    "#### Model description\n",
    "YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n",
    "\n",
    "The model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n",
    "\n",
    "#### Intended uses & limitations\n",
    "You can use the raw model for object detection. See the model hub to look for all available YOLOS models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3401126-756b-4394-930c-c833c1f574b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "detector = pipeline(task=\"object-detection\",model= model_path + \"yolos-tiny\",local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32dc65d4-3868-4d10-a433-9f50832e8e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9895,\n",
       "  'label': 'dog',\n",
       "  'box': {'xmin': 345, 'ymin': 10, 'xmax': 995, 'ymax': 1140}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = detector(\n",
    "    \"https://pic.rmb.bdstatic.com/bjh/news/8a7aa17a4d5bd3ed4db4fb1182e8b350.png@s_0,w_1242\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550759be-93ab-4988-81f0-c0cb8eccfda8",
   "metadata": {},
   "source": [
    "![](data/image/cat_dog.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f66cd7-f2c8-4af3-b9ab-60012792ec1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9946,\n",
       "  'label': 'cat',\n",
       "  'box': {'xmin': 75, 'ymin': 60, 'xmax': 290, 'ymax': 369}},\n",
       " {'score': 0.9899,\n",
       "  'label': 'dog',\n",
       "  'box': {'xmin': 280, 'ymin': 18, 'xmax': 479, 'ymax': 416}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = detector(\n",
    "    \"data/image/cat_dog.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
