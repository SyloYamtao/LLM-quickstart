{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5969cc4b-505f-4a3b-ad02-18aa1e34b3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU device name: NVIDIA GeForce RTX 4090\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "\n",
    "# 检查是否可以访问 CUDA\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(datasets_path+\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': \"I'm not a huge fan of this location. I think that it was oddly built- the small, alley-like front makes it difficult to get past people on your way to/from the bathroom or tables when it's busy. And there's hardly any tables to sit at. Furthermore, people tend to clog the front on their way in, which makes things particularly difficult (especially in the winter weather). The staff were pretty impersonal to be, but maybe that was due to the high traffic of the place and the time I was there. And the coffee that I had was cold- I'm sure it was probably the bottom of the batch. I'd probably only walk in here again if someone else suggested it before or after a movie or while we were shopping in the area.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=20):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>If you go to Cave Creek you must go to the Buffalo Chip, Harold's , hideaway etc.  just great local restaurants in a great town north of Phoenix/Scottsdale.   I like the Chip more for drinking dancing and rodeo than the food. But it is okay for a little grub   But it is fun.  So try it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Chuy's was pretty good.  Seems like all of their locations are pretty similar.  I think they'd be best known for their cheap margaritas... $2 pints, $4 small pitchers.  They aren't too strong and come from a mediocre mix, but if you're looking for something sweet and cheap its a good bet.\\n\\nFree serve-yourself chips and salsa are pretty good.  \\n\\nAs for food... pretty decent prices and okay to good food.  I wouldn't say its authentic Mexican, actually I think the menu is a bit confused.  But the food is good overall.  It's a place we'll go to every month or so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>I ended up eating at Taggia while staying at Firesky so it was a choice of convenience. I've had the food from here several times using room service and it's never anything to complain about. It was the same story the day I had lunch here. I had an organic greens salad and shared the margherita and goat cheese pizzas with my fellow lunchers. All of the food was good - the goat cheese pizza in particular with its thin, crispy crust.\\n\\nUnfortunately the day we ate here our service was MIA. We were told we could seat ourselves so we did. After about 10 minutes someone came by to take our drink order and maybe 10 minutes later our waters arrived. Well 2 out of 3 of them did anyway. Then we ordered two salads and two pizzas to share. One pizza came first. WTH? Where were the salads? Or the other pizza? The salads showed up a few minutes later and then our server realized that she had forgotten our second pizza. No biggie since we had salads and one pizza to eat. But the service was lackluster with a L. Like Andrea R says, I wouldn't go out of my way to eat here, but when in the area it's a good option to have.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I recently had a work luncheon at Ricardo's, I had been before years ago and it was extremely unmemorable. This visit would be more memorable but for the wrong reasons. \\n\\nWhen given the choice, I prefer to order off the menu than choose a buffet. But the whole group went to the buffet and I didn't want to be the oddball. I had two carne asada tacos, cheese enchilada and chips &amp; salsa. The enchilada was bland the only hint of flavor was the acidity from the tomatoes. The salsa, too, was bland and watery. The chips were pretty generic. The first taco was ok, a bit bland, but tender. The second was filled with grizzly meat. It really turned my stomach. Fortunately, the service was friendly and they were able to accomodate our large group.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>We had a great time at this resort over the long weekend.  The staff was super friendly, especially Adam, David and Cassie.  Great job!!!  And our suite was perfect to accommodate three women with lots of bags, make-up and shoes.  The Hole in the Wall restaurant had a really good breakfast, friendly staff and an outdoors patio.  Not so for the Rico Restaurant.  They were a bit rude, overwhelmed and obviously didn't want our business.  We also floated down the Lazy River, it was definitely Lazy...pretty slow but perfect temp.  All in all, I'll be back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Im an owner with no kids, this place is not for my husband and I.. The element here is all about families and cooking in and playing in the pool from the moment it opens.\\n\\nThe restaurant bar is a bit of a joke, and the pressure to buy more points makes a relaxing vacation more stressful. We were an original owner and saw most of it built.\\n\\nWe noticed that they no longer offer a shuttle which is a mistake for those that want to go to the strip and not have to worry about driving. But after this weekend I see that they don't need to offer the shuttle because more than half the people there don't plan on leaving the facility at all.\\n\\nThe guests that we ran into all seemed to be there on a free vacation offer. They were tatted up and pushing baby strollers... and screaming to each other WAIT TIL YOU SEE THE POOL....\\n\\nMy hubby and myself both looked at each other and said OUT LOUD, we don't think we will be coming back here again ever to this location.\\n\\nWe came home and looked into selling  it all together, but then thought maybe we would try another location that Diamond Resorts has to offer before we do so..\\n\\nSo bottom line, if you have kids and love the pool and slides and pool some more.. this is for you.. If your looking for a weekend with the hubby or friends in Vegas to relax and to enjoy what Vegas is all about... this resort is not for you..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Booked a room here through Priceline for the Tuesday before Thanksgiving.  Actually booked it on the drive in from Las Vegas through my cell phone, which was pretty sweet.  Paid $25 + tax, so you can't beat the price.  We had a hard time finding it as Google Maps was wrong about it's location, but you can't blame the hotel for that.\\n\\nWhat I can blame the hotel for is not giving me a king size bed.  Priceline had booked me with 2 doubles, and in my experience I am always able to switch unless they're sold out.  The front desk clerk told me they were indeed not sold out, but it was their policy not to let Priceline users switch rooms.  So much for considering staying there the rest of Thanksgiving week.\\n\\nI was going to let it go, but then at 9:30am the maid knocked on our door and woke me up 2 hours earlier than I had planned (we got to sleep at 5am, give me a break).  I ended up coming out of my room and saw that my do not disturb sign was on, so she must have chosen to ruin my day for fun.  Tried to fall back asleep but was then kept up by the sounds of what looked to be a loud garbage truck parked right outside of our room.\\n\\nI give up on sleeping.  At least they have solid free Internet so I can Yelp this hotel.  Courtyard, you're lucky you're getting 3 stars from me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Been to 4 Cirques and this is the least favorite.  Sets and costumes are absolutely amazing but the acts were very unimpressive compared to the older ones we've seen.  \\nThe only exception was the opening act with the two twin men on ropes that swung out into the audience.  They weren't in the book at the shop so I think they added them in later to spice up the program. Breathtaking!\\n\\nPros- Art direction, sets, lights\\n\\nCons- Acts seen before and ANNOYING clowns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2 star</td>\n",
       "      <td>KOOLAID KID reminded me of home...nice touch..i know I know..this is suppose to be about the chicken &amp; waffles, but I must say quenching my thirst is very important to me..so back to the food..it was just that chicken(no flavor) &amp; waffles(nothing special)..mac &amp; cheese was very nice...and the new building was very very nice..okay that's all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Just called this location and I live 1.8 miles away. I asked them to deliver and they informed me that they would not deliver to my house because it was a couple hundred yards out of the map plan. They asked me to call the power and southern store. This store advised me that they could not deliver because jimmy johns has a two mile radius they can deliver to.  Called this store back and they once again decided to tell me even though I was in the two mile radius they did not want to deliver to me and my only option was for pickup. I will never eat at this location. I know the owners at Firehouse Subs and they go out of the way and this location is just lazy. Not getting my money jimmy johns no matter how fast you are. Laziness is worse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Officially my favorite steakhouse hands down!  As if Vegas doesn't have enough to write about, this little gem is in one of the remaining standouts of what some call \\\"Old Vegas\\\".  The meal got off to a great start at the bar as we waited for our table.  The service was fast, the drinks were well made and naturally they were top shelf.  I was particularly pleased with the professional look and feel of the place.  Its very old school, mafioso, dark, white linen, with the sounds of clinking wine glasses and lively conversation in the background.  \\n\\nThe food:  Excellent!  Your steak is served as intended, well aged beef cooked to order on a hot plate...nothing else!  Fantastic.  The sides are of sharable portions so agree on one for every 2 or 3 people unless you plan on toting a box of leftovers around Vegas.  The mac and cheese is perfect as is the smooth and creamy lobster bisque.  The other members in my party had several cuts of meat and one had a chicken dish...my sincerest apologies for not having info on those.  Fact is, I wanted every square inch of space in my belly for the rib eye on my plate.  Like sooo many other reviews about this place, it was melt in your mouth good.  We also ordered a chocolate gnash desert that could only be made better by eating it off of your favorite super models body, otherwise it was absolute perfection.  \\n\\nService was superb.  Our waiter was attentive, informative and practiced.  He was professional and friendly.  Over all a fantastic experience.  I will definitely be returning.  Highly recommend this location to anyone.  You will not be disappointed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Horrible college! If you can go somewhere else they are so unorganized and try to find ways to make you attend their school.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>We love to get take out at Los Taquitos on a busy week night when we are too tired to cook. They are super fast and dinner for 4 is usually under $20! \\n\\nI love the street tacos and my husband the ceviche. Son loves the carne asada burrito, daughter the tamales. Seriously something for everyone. There IS some variability in the spicy factor, but we are total regulars. Gotta try it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Stay far away from here!! I had braces put on by them and a few years later my teeth are still messed up! They screwed up my mouth so bad that every dentist I've gone to refuses to touch me until I have jaw surgery due to Western Dental's negligence. Then they sent me to collections for no reason which it has since been taken care of but that just shows how  negligent they are!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>I love this place. It's the best hipster dive in Las Vegas! You can get deep fried Oreos.. For real. The girls are always super friendly and the drinks are fast and strong. What else could you want?!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1 star</td>\n",
       "      <td>CLOSED.....These guys lasted about two months...they're gone now, replaced by Zza's Pizza...will review it once I've tried it. But I gotta say, it's a funky location, and location's everything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1 star</td>\n",
       "      <td>We just left this restaurant.  I can truthfully say the food was amazing. We had two servers who were fabulous. The manager however needs to reconsider her job choice. We made a reservation for a group of 15. We were hoping to go at 7 but they asked us to come at 6:30. Fine, we did.  When we for there they were not prepared for our group. We pulled the tables and chairs ourselves. Then after all 13 of us had ordered, the manager came over and said they only had 2 woks going in the kitchen and could we combine orders. We then took our order down to 10 orders. We of course ended up adding more dishes. The manager spent the rest of our stay glaring in our direction. Then the bill came...wow. They put extra food on our bill. When we told them about it, she was condescending and made it sound as if we were lying and then as if she would punish the kitchen for sending us food we didn't order or eat. We paid, making sure we counted out every cent of the money to her so there would be no confusion. We walked out of there and straight into a bar at the container park to wash the taste of this horrible customer service off our palates. Some of our party had been here before and spoke of how great it was...sadly none of us plan on returning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>After reading reviews, I went to Taco Haus with medium expectations; however, I was quite happy with the results. The service was terrific and the food was very good. \\n\\nI enjoyed the raw jicama salad as well as the rice (I requested it with butter), guacamole, veggie taco (I requested it with butter instead of veggie oil), and the pork belly taco was pretty good. My special diet requests which were taken care of with ease and that made me extra pleased. \\n\\nTaco Haus sources local ingredients with what seems to be mostly organic. That alone will bring me back again. Their meat is hormone free, but if it were also grass fed, they'd get that much more of my business.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Okay, so I am a sucker for Panda Express.  You may think it is not Chinese food, but I gotta tell you that their Orange Chicken is to die for.  The store at Thunderbird and 40th Street is fast (usually) and offers hot and fresh food.  Expect to stand in line (out the door) if you arrive between 5-6 p.m. on weekdays.  Overall, a quick place to get chinese food served fresh and hot.  My favs:  Orange Chicken, Chicken with Mushroom, and Mandarin Chicken.  Tip:  if you are on a diet, buy the kids meal.  It comes with one side and one entree, plus a cookie and a drink.  Give the cookie to your kid, order Ice Tea or Diet Coke, and enjoy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1 star</td>\n",
       "      <td>\\\"Eek! Methinks not\\\" is RIGHT! (one star because we couldn't go lower)\\nI'm writing this as my family and I are cracking up in horror, disbelief, and embarrassment.  We saw the show an hour ago and can't stop making fun.  We were so NOT entertained, the only way to make it worth our money is to laugh our asses off at how incredibly stupid and awkward this show was.  We had a party of 12 people and here's what each said:\\n9 year old boy #1:  I thought it would be more magical.  Is the fiance from Hooters?\\n9 year old boy #2:  My teenage cousin is better at magic in our living room. Laaaa--        aaaaaame!\\n10 year old girl: The show was more about HIM than him doing magic. I wanted to                     put a paper bag over my head.  Not that the person himself stinks, he just stinks at magic...\\n11 year old girl: Are we at the right show? \\n12 year old girl:  I'm ashamed of/for the mother (aka DJ M-O-M also dressed like Hooters girl).\\n12 year old boy: The mom looks like a hooker and the fiance IS a hooker (and we didn't even know he knew the word hooker)\\n14 year old boy: It was good huhuhuhuhuhuh\\n16 year old boy:  Mom, if you EVER wore that...\\nmom #1: Why in God's good name would we want to see him lip sync an Elvis song while waltzing down the aisle shaking hands like an actual performer? Or play the drums with his one-tune dad?  He was AVOIDING magic.  \\nmom#2: The finger snapping is awkward and creeping me out.  The stage manager looks mortified and MUST be a cousin working to earn his keep.  I was uncomfortable.\\ndad#1: Tommy Wind is magic's answer to the Wolf of Wall Street. Goooooooooong\\ndad #2: no comment (in fear of bad karma...)\\n\\nWe simply could not make eye contact at the end.  Meet and greet--me thinks NOT. We're too ashamed. \\n\\nOn a serious note, the positive reviews either have to be from friends and family, or we saw an extremely OFF night...For some constructive criticism--Tommy Wind's passion for show business is nice.  Hopefully, he will perfect some tricks (we could see some of the tricks we shouldn't have seen) and focus more on magic than the music portion.  We paid less for the Nathan Burton show last year and it was very good, so we expected some good illusions and slight of hand. Also, It's nice to work with family, but maybe he can work on integrating the family stories more smoothly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "### google-bert/bert-base-cased\n",
    "#### BERT base model (cased)\n",
    "Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model is case-sensitive: it makes a difference between english and English.\n",
    "\n",
    "Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by the Hugging Face team.\n",
    "\n",
    "#### Model description\n",
    "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with two objectives:\n",
    "\n",
    "Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n",
    "Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to predict if the two sentences were following each other or not.\n",
    "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the BERT model as inputs.\n",
    "\n",
    "#### Intended uses & limitations\n",
    "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n",
    "\n",
    "Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path+\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Just an update, just adding another star for the apology they gave us.....</td>\n",
       "      <td>[101, 2066, 1126, 11984, 117, 1198, 5321, 1330, 2851, 1111, 1103, 13382, 1152, 1522, 1366, 119, 119, 119, 119, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path+\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased-finetune-yelp\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=5,\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased-finetune-yelp/runs/Jun09_06-29-13_notebook-79fd7c8d-5ca0-4293-a778-90491560c164-0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased-finetune-yelp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased-finetune-yelp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec1846b-3178-49d7-957e-e7792dd9cc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate                  0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1010a4de-c7d4-4b86-a342-062300bf39f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.591947</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.477073</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.407272</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=1.507139383951823, metrics={'train_runtime': 15.1283, 'train_samples_per_second': 39.661, 'train_steps_per_second': 4.958, 'total_flos': 157870885478400.0, 'train_loss': 1.507139383951823, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Prepare and tokenize dataset\n",
    "dataset = load_dataset(datasets_path+\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path+\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Setup evaluation \n",
    "metric = evaluate.load(\"metrics/accuracy/\"+\"accuracy.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Load pretrained model and evaluate model after each epoch\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path+\"bert-base-cased\", num_labels=5)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"metrics/accuracy/\"+\"accuracy.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.617500</td>\n",
       "      <td>1.518585</td>\n",
       "      <td>0.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>1.125827</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.941400</td>\n",
       "      <td>1.004606</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=1.2967164125392046, metrics={'train_runtime': 60.5968, 'train_samples_per_second': 49.508, 'train_steps_per_second': 3.119, 'total_flos': 789354427392000.0, 'train_loss': 1.2967164125392046, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.058894395828247,\n",
       " 'eval_accuracy': 0.51,\n",
       " 'eval_runtime': 0.5723,\n",
       " 'eval_samples_per_second': 174.733,\n",
       " 'eval_steps_per_second': 22.715,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd92e35d-fed7-4ff2-aa84-27b5e29b917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {},
   "source": [
    "## Homework: 使用完整的 YelpReviewFull 数据集训练，看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf2cc6b-7b4f-49f2-b424-e7765f54b60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dataDisk/code-workspace/LLM-quickstart/transformers'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f7fb4-494c-489d-b151-b00c5616b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "### per_device_train_batch_size =16\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path+\"bert-base-cased\", num_labels=5)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30,\n",
    "                                  save_total_limit=3)\n",
    "\n",
    "Thu Jun 13 03:03:29 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:12:00.0 Off |                  Off |\n",
    "| 30%   51C    P2             300W / 450W |  12205MiB / 24564MiB |     75%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "+---------------------------------------------------------------------------------------+\n",
    "\n",
    " [121875/121875 02:09, Epoch 3/3]\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\n",
    "3\t0.518700\t0.754804\t0.688700\n",
    " [1250/1250 00:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d4331-30e5-4a7e-9a80-c5a579430534",
   "metadata": {},
   "outputs": [],
   "source": [
    "### per_device_train_batch_size =24\n",
    "Thu Jun 13 03:18:56 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:12:00.0 Off |                  Off |\n",
    "| 74%   57C    P2             391W / 450W |  17347MiB / 24564MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbebafd-9f4d-4fd3-8b64-638d9b8e23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thu Jun 13 03:21:44 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:12:00.0 Off |                  Off |\n",
    "| 72%   59C    P2             388W / 450W |  22547MiB / 24564MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee2580a-7a5a-46ae-a28b-b41e9e838eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60939' max='60939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60939/60939 7:06:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.769700</td>\n",
       "      <td>0.729502</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.635500</td>\n",
       "      <td>0.703414</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.560500</td>\n",
       "      <td>0.738425</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer_24/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer_24/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 00:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "\n",
    "# Prepare and tokenize dataset\n",
    "dataset = load_dataset(datasets_path+\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path+\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# Setup evaluation \n",
    "metric = evaluate.load(\"metrics/accuracy/\"+\"accuracy.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Load pretrained model and evaluate model after each epoch\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path+\"bert-base-cased\", num_labels=5)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer_24\", \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30,\n",
    "                                  save_total_limit=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=all_train_dataset,\n",
    "    eval_dataset=all_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "trainer.save_model(\"test_trainer_24\")\n",
    "\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b969d3-58b7-4cfd-8808-ef1dfa4bcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thu Jun 13 10:41:21 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:12:00.0 Off |                  Off |\n",
    "| 30%   27C    P8              13W / 450W |  22547MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "+---------------------------------------------------------------------------------------+\n",
    "\n",
    "Thu Jun 13 12:09:26 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:12:00.0 Off |                  Off |\n",
    "| 32%   56C    P2             291W / 450W |  17209MiB / 24564MiB |     82%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727dd45b-b9a6-4ff2-ad18-6fc90a625b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:12<00:00, 3963.62 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50780' max='50780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50780/50780 7:36:38, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.712930</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.743686</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.889443</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# 模型和数据集的路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_32_liner\"\n",
    "\n",
    "# 加载并标记数据集\n",
    "# 加载Yelp评论数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")  \n",
    "# 加载BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\") \n",
    "\n",
    "# 定义标记函数\n",
    "def tokenize_function(examples):\n",
    "    # 对每个样本进行tokenization，填充和截断\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)  \n",
    "\n",
    "# 对数据集进行标记\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True) \n",
    "\n",
    "# 准备训练集和验证集\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))  # 从训练集中随机选择65万个样本\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))  # 从测试集中随机选择1万个样本用于验证\n",
    "\n",
    "# 评估函数\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")  # 加载准确率评估函数\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # 分离预测值和真实标签\n",
    "    predictions = np.argmax(logits, axis=-1)  # 取预测值的最大概率对应的标签\n",
    "    return metric.compute(predictions=predictions, references=labels)  # 计算准确率\n",
    "\n",
    "# 加载预训练模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)  # 加载BERT模型，并指定输出标签的数量\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=\"epoch\",  # 每个epoch后进行评估\n",
    "    save_strategy=\"epoch\",  # 每个epoch后保存模型\n",
    "    learning_rate=3e-5,  # 学习率\n",
    "    per_device_train_batch_size=32,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=5,  # 训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=2,  # 梯度累积步骤，模拟更大的batch size\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"linear\",  # 使用线性学习率调度器\n",
    "    warmup_steps=500  # 热身步数，逐步增加学习率\n",
    ")\n",
    "\n",
    "# 创建Trainer实例\n",
    "trainer = Trainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练集\n",
    "    eval_dataset=all_eval_dataset,  # 验证集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和训练状态\n",
    "trainer.save_model(result_output_dir)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d5bb3a-b017-4950-9a22-a43052b45245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:12<00:00, 3963.62 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50780' max='50780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50780/50780 7:36:38, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.712930</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.743686</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.889443</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# 模型和数据集的路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_32_liner\"\n",
    "\n",
    "# 加载并标记数据集\n",
    "# 加载Yelp评论数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")  \n",
    "# 加载BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\") \n",
    "\n",
    "# 定义标记函数\n",
    "def tokenize_function(examples):\n",
    "    # 对每个样本进行tokenization，填充和截断\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)  \n",
    "\n",
    "# 对数据集进行标记\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True) \n",
    "\n",
    "# 准备训练集和验证集\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))  # 从训练集中随机选择65万个样本\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))  # 从测试集中随机选择1万个样本用于验证\n",
    "\n",
    "# 评估函数\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")  # 加载准确率评估函数\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # 分离预测值和真实标签\n",
    "    predictions = np.argmax(logits, axis=-1)  # 取预测值的最大概率对应的标签\n",
    "    return metric.compute(predictions=predictions, references=labels)  # 计算准确率\n",
    "\n",
    "# 加载预训练模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)  # 加载BERT模型，并指定输出标签的数量\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=\"epoch\",  # 每个epoch后进行评估\n",
    "    save_strategy=\"epoch\",  # 每个epoch后保存模型\n",
    "    learning_rate=3e-5,  # 学习率\n",
    "    per_device_train_batch_size=32,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=5,  # 训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=2,  # 梯度累积步骤，模拟更大的batch size\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"linear\",  # 使用线性学习率调度器\n",
    "    warmup_steps=500  # 热身步数，逐步增加学习率\n",
    ")\n",
    "\n",
    "# 创建Trainer实例\n",
    "trainer = Trainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练集\n",
    "    eval_dataset=all_eval_dataset,  # 验证集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和训练状态\n",
    "trainer.save_model(result_output_dir)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af75a4c-3a05-4c6a-88a5-8ae5c6923e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10728' max='101560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10728/101560 1:35:52 < 13:31:57, 1.86 it/s, Epoch 1.06/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.722440</td>\n",
       "      <td>0.679500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset  # 导入用于加载数据集的函数\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  # 导入所需的transformers类\n",
    "import numpy as np  # 导入numpy用于数组操作\n",
    "import evaluate  # 导入用于评估的库\n",
    "\n",
    "# 模型和数据集的路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_32_liner_1\"\n",
    "\n",
    "# 加载并标记数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")  # 加载Yelp评论数据集\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\")  # 加载BERT tokenizer\n",
    "\n",
    "# 定义标记函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)  # 对每个样本进行tokenization，填充和截断\n",
    "\n",
    "# 对数据集进行标记\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)  # 对整个数据集进行标记\n",
    "\n",
    "# 准备训练集和验证集\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))  # 从训练集中随机选择65万个样本\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))  # 从测试集中随机选择1万个样本用于验证\n",
    "\n",
    "# 评估函数\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")  # 加载准确率评估函数\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # 分离预测值和真实标签\n",
    "    predictions = np.argmax(logits, axis=-1)  # 取预测值的最大概率对应的标签\n",
    "    return metric.compute(predictions=predictions, references=labels)  # 计算准确率\n",
    "\n",
    "# 加载预训练模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)  # 加载BERT模型，并指定输出标签的数量\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=\"epoch\",  # 每个epoch后进行评估\n",
    "    save_strategy=\"epoch\",  # 每个epoch后保存模型\n",
    "    learning_rate=2e-5,  # 降低学习率\n",
    "    per_device_train_batch_size=32,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=10,  # 增加训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=2,  # 梯度累积步骤，模拟更大的batch size\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"linear\",  # 使用线性学习率调度器\n",
    "    warmup_steps=500,  # 热身步数，逐步增加学习率\n",
    "    logging_dir=\"logs\",  # 日志记录路径\n",
    "    report_to=\"tensorboard\"  # 使用TensorBoard记录训练过程\n",
    ")\n",
    "\n",
    "# 创建Trainer实例\n",
    "trainer = Trainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练集\n",
    "    eval_dataset=all_eval_dataset,  # 验证集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和训练状态\n",
    "trainer.save_model(result_output_dir)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4bf021-7af3-4ef2-85a2-96a4571fb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46431' max='46431' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46431/46431 4:39:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.705400</td>\n",
       "      <td>0.708036</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.705144</td>\n",
       "      <td>0.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.542100</td>\n",
       "      <td>0.731232</td>\n",
       "      <td>0.695900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0:\n",
      "  Training Loss = 0.7054\n",
      "  Validation Loss = 0.7080358266830444\n",
      "  Accuracy = 0.6856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2.0:\n",
      "  Training Loss = 0.6235\n",
      "  Validation Loss = 0.7051438689231873\n",
      "  Accuracy = 0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3.0:\n",
      "  Training Loss = 0.5421\n",
      "  Validation Loss = 0.7312320470809937\n",
      "  Accuracy = 0.6959\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3.0:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_eval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 保存模型和状态\u001b[39;00m\n\u001b[1;32m     89\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(result_output_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3123\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   3120\u001b[0m     \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m     xm\u001b[38;5;241m.\u001b[39mmaster_print(met\u001b[38;5;241m.\u001b[39mmetrics_report())\n\u001b[0;32m-> 3123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstop_and_update_metrics(output\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_callback.py:396\u001b[0m, in \u001b[0;36mCallbackHandler.on_evaluate\u001b[0;34m(self, args, state, control, metrics)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics):\n\u001b[1;32m    395\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_evaluate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_callback.py:414\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m, in \u001b[0;36mLoggingCallback.on_evaluate\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;241m.\u001b[39mlog_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Validation Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;241m.\u001b[39mlog_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;241m.\u001b[39mlog_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, IntervalStrategy\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers.optimization import AdamW\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# 自定义回调函数，在每个epoch结束后输出结果\n",
    "class LoggingCallback(TrainerCallback):\n",
    "       def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}:\")\n",
    "        # 检查并打印训练损失\n",
    "        if len(state.log_history) > 1 and 'loss' in state.log_history[-2]:\n",
    "            print(f\"  Training Loss = {state.log_history[-2]['loss']}\")\n",
    "        else:\n",
    "            print(\"  Training Loss not available\")\n",
    "        # 检查并打印验证损失\n",
    "        if 'eval_loss' in state.log_history[-1]:\n",
    "            print(f\"  Validation Loss = {state.log_history[-1]['eval_loss']}\")\n",
    "        else:\n",
    "            print(\"  Validation Loss not available\")\n",
    "        # 检查并打印准确率\n",
    "        if 'eval_accuracy' in state.log_history[-1]:\n",
    "            print(f\"  Accuracy = {state.log_history[-1]['eval_accuracy']}\")\n",
    "        else:\n",
    "            print(\"  Accuracy not available\")\n",
    "\n",
    "# 自定义 Trainer 类，使用 AdamW 优化器\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        return self.optimizer\n",
    "\n",
    "# 设置模型和数据集路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_adamW\"\n",
    "\n",
    "# 准备和标记数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# 设置评估指标\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 加载预训练模型，并指定输出标签的数量\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,  # 每个epoch后进行评估\n",
    "    save_strategy=IntervalStrategy.EPOCH,  # 每个epoch后保存模型\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    per_device_train_batch_size=42,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=42,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=3,  # 训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=1,  # 梯度累积步骤，减少一半以保持总批次大小相同\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"linear\",  # 使用线性学习率调度器\n",
    "    warmup_steps=500,  # 热身步数，逐步增加学习率\n",
    "    logging_dir='./logs',  # 日志记录路径\n",
    "    report_to=\"none\"  # 不使用TensorBoard记录日志\n",
    ")\n",
    "\n",
    "# 创建自定义 Trainer 实例\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练数据集\n",
    "    eval_dataset=all_eval_dataset,  # 验证数据集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    "    callbacks=[LoggingCallback()]  # 添加自定义回调函数\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和状态\n",
    "trainer.save_model(result_output_dir)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c90838-c165-42fb-9221-b91a3fe6c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77385' max='77385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77385/77385 4:39:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>0.733922</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>0.777048</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.837755</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3.0:\n",
      "  Training Loss = 0.5749\n",
      "  Validation Loss = 0.7339217066764832\n",
      "  Accuracy = 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer_adamW/checkpoint-46431 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4.0:\n",
      "  Training Loss = 0.5337\n",
      "  Validation Loss = 0.7770480513572693\n",
      "  Accuracy = 0.6889\n",
      "Epoch 5.0:\n",
      "  Training Loss = 0.4521\n",
      "  Validation Loss = 0.8377554416656494\n",
      "  Accuracy = 0.6881\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5.0:\n",
      "  Training Loss not available\n",
      "  Validation Loss = 0.7051438689231873\n",
      "  Accuracy = 0.6906\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, IntervalStrategy\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers.optimization import AdamW\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# 自定义回调函数，在每个epoch结束后输出结果\n",
    "class LoggingCallback(TrainerCallback):\n",
    "       def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}:\")\n",
    "        # 检查并打印训练损失\n",
    "        if len(state.log_history) > 1 and 'loss' in state.log_history[-2]:\n",
    "            print(f\"  Training Loss = {state.log_history[-2]['loss']}\")\n",
    "        else:\n",
    "            print(\"  Training Loss not available\")\n",
    "        # 检查并打印验证损失\n",
    "        if 'eval_loss' in state.log_history[-1]:\n",
    "            print(f\"  Validation Loss = {state.log_history[-1]['eval_loss']}\")\n",
    "        else:\n",
    "            print(\"  Validation Loss not available\")\n",
    "        # 检查并打印准确率\n",
    "        if 'eval_accuracy' in state.log_history[-1]:\n",
    "            print(f\"  Accuracy = {state.log_history[-1]['eval_accuracy']}\")\n",
    "        else:\n",
    "            print(\"  Accuracy not available\")\n",
    "\n",
    "# 自定义 Trainer 类，使用 AdamW 优化器\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        return self.optimizer\n",
    "\n",
    "# 设置模型和数据集路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_adamW/\"\n",
    "\n",
    "# 准备和标记数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# 设置评估指标\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 加载预训练模型，并指定输出标签的数量\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,  # 每个epoch后进行评估\n",
    "    save_strategy=IntervalStrategy.EPOCH,  # 每个epoch后保存模型\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    per_device_train_batch_size=42,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=42,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=5,  # 训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=1,  # 梯度累积步骤，减少一半以保持总批次大小相同\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"linear\",  # 使用线性学习率调度器\n",
    "    warmup_steps=500,  # 热身步数，逐步增加学习率\n",
    "    logging_dir='./logs',  # 日志记录路径\n",
    "    report_to=\"none\"  # 不使用TensorBoard记录日志\n",
    ")\n",
    "\n",
    "# 创建自定义 Trainer 实例\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练数据集\n",
    "    eval_dataset=all_eval_dataset,  # 验证数据集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    "    callbacks=[LoggingCallback()]  # 添加自定义回调函数\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train(result_output_dir+\"checkpoint-30954\")\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和状态\n",
    "trainer.save_model(result_output_dir)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ff65d-668c-4775-acb0-d96bb58411bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fri Jun 14 12:26:51 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:12:00.0 Off |                  Off |\n",
    "| 31%   56C    P2             295W / 450W |  23245MiB / 24564MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21a1858-3518-41fe-ade5-42e31b2b1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/root/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77380' max='77380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77380/77380 6:03:13, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.719761</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.719231</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>0.719111</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6.999935387995089:\n",
      "  Training Loss = 0.6724\n",
      "  Validation Loss = 0.7197611927986145\n",
      "  Accuracy = 0.6881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8.0:\n",
      "  Training Loss = 0.6665\n",
      "  Validation Loss = 0.7198094129562378\n",
      "  Accuracy = 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8.99993538799509:\n",
      "  Training Loss = 0.6517\n",
      "  Validation Loss = 0.7192310690879822\n",
      "  Accuracy = 0.6893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9.999741551980359:\n",
      "  Training Loss = 0.6505\n",
      "  Validation Loss = 0.7191110253334045\n",
      "  Accuracy = 0.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9.999741551980359:\n",
      "  Training Loss not available\n",
      "  Validation Loss = 0.718285858631134\n",
      "  Accuracy = 0.6872\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, IntervalStrategy\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers.optimization import AdamW\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# 自定义回调函数，在每个epoch结束后输出结果\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}:\")\n",
    "        # 检查并打印训练损失\n",
    "        if len(state.log_history) > 1 and 'loss' in state.log_history[-2]:\n",
    "            print(f\"  Training Loss = {state.log_history[-2]['loss']}\")\n",
    "        else:\n",
    "            print(\"  Training Loss not available\")\n",
    "        # 检查并打印验证损失\n",
    "        if 'eval_loss' in state.log_history[-1]:\n",
    "            print(f\"  Validation Loss = {state.log_history[-1]['eval_loss']}\")\n",
    "        else:\n",
    "            print(\"  Validation Loss not available\")\n",
    "        # 检查并打印准确率\n",
    "        if 'eval_accuracy' in state.log_history[-1]:\n",
    "            print(f\"  Accuracy = {state.log_history[-1]['eval_accuracy']}\")\n",
    "        else:\n",
    "            print(\"  Accuracy not available\")\n",
    "\n",
    "# 自定义 Trainer 类，使用 AdamW 优化器\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        return self.optimizer\n",
    "\n",
    "# 设置模型和数据集路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_adamW_1/\"\n",
    "\n",
    "# 准备和标记数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# 设置评估指标\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 加载预训练模型，并指定输出标签的数量\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,  # 每个epoch后进行评估\n",
    "    save_strategy=IntervalStrategy.EPOCH,  # 每个epoch后保存模型\n",
    "    learning_rate=2e-6,  # 学习率\n",
    "    per_device_train_batch_size=128,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=128,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=10,  # 训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=2,  # 梯度累积步骤\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"cosine\",  # 使用余弦学习率调度器\n",
    "    warmup_steps=1000,  # 热身步数，逐步增加学习率\n",
    "    logging_dir='./logs',  # 日志记录路径\n",
    "    report_to=\"none\"  # 不使用TensorBoard记录日志\n",
    ")\n",
    "\n",
    "# 创建自定义 Trainer 实例\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练数据集\n",
    "    eval_dataset=all_eval_dataset,  # 验证数据集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    "    callbacks=[LoggingCallback()]  # 添加自定义回调函数\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train(result_output_dir+\"checkpoint-46430\")\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和状态\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52645c41-498d-4386-8ca7-a8cb0129be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/dataDisk/hf/hub/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/root/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3357' max='12695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3357/12695 1:49:30 < 5:04:47, 0.51 it/s, Epoch 1.32/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>0.871679</td>\n",
       "      <td>0.611700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.9999015457320075:\n",
      "  Training Loss = 0.8974\n",
      "  Validation Loss = 0.8716785907745361\n",
      "  Accuracy = 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, IntervalStrategy\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers.optimization import AdamW\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# 自定义回调函数，在每个epoch结束后输出结果\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}:\")\n",
    "        # 检查并打印训练损失\n",
    "        if len(state.log_history) > 1 and 'loss' in state.log_history[-2]:\n",
    "            print(f\"  Training Loss = {state.log_history[-2]['loss']}\")\n",
    "        else:\n",
    "            print(\"  Training Loss not available\")\n",
    "        # 检查并打印验证损失\n",
    "        if 'eval_loss' in state.log_history[-1]:\n",
    "            print(f\"  Validation Loss = {state.log_history[-1]['eval_loss']}\")\n",
    "        else:\n",
    "            print(\"  Validation Loss not available\")\n",
    "        # 检查并打印准确率\n",
    "        if 'eval_accuracy' in state.log_history[-1]:\n",
    "            print(f\"  Accuracy = {state.log_history[-1]['eval_accuracy']}\")\n",
    "        else:\n",
    "            print(\"  Accuracy not available\")\n",
    "\n",
    "# 自定义 Trainer 类，使用 AdamW 优化器\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        return self.optimizer\n",
    "\n",
    "# 设置模型和数据集路径\n",
    "model_path = \"/root/dataDisk/hf/hub/models/\"\n",
    "datasets_path = \"/root/dataDisk/hf/hub/datasets/\"\n",
    "result_output_dir = \"test_trainer_adamW_2/\"\n",
    "\n",
    "# 准备和标记数据集\n",
    "dataset = load_dataset(datasets_path + \"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "all_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(650000))\n",
    "all_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# 设置评估指标\n",
    "metric = evaluate.load(\"metrics/accuracy/\" + \"accuracy.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 加载预训练模型，并指定输出标签的数量\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path + \"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=result_output_dir,  # 模型输出路径\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,  # 每个epoch后进行评估\n",
    "    save_strategy=IntervalStrategy.EPOCH,  # 每个epoch后保存模型\n",
    "    learning_rate=1e-6,  # 学习率\n",
    "    per_device_train_batch_size=32,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=5,  # 训练的epoch数量\n",
    "    logging_steps=30,  # 每30步记录一次日志\n",
    "    save_total_limit=3,  # 最多保存3个模型检查点\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载表现最好的模型\n",
    "    gradient_accumulation_steps=4,  # 梯度累积步骤\n",
    "    fp16=True,  # 启用混合精度训练，减少显存占用\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # 余弦学习率调度器\n",
    "    warmup_steps=1000,  # 热身步数，逐步增加学习率\n",
    "    logging_dir='./logs',  # 日志记录路径\n",
    "    report_to=\"none\"  # 不使用TensorBoard记录日志\n",
    ")\n",
    "\n",
    "# 创建自定义 Trainer 实例\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # 模型\n",
    "    args=training_args,  # 训练参数\n",
    "    train_dataset=all_train_dataset,  # 训练数据集\n",
    "    eval_dataset=all_eval_dataset,  # 验证数据集\n",
    "    compute_metrics=compute_metrics,  # 评估函数\n",
    "    callbacks=[LoggingCallback()]  # 添加自定义回调函数\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 评估模型\n",
    "trainer.evaluate(all_eval_dataset)\n",
    "\n",
    "# 保存模型和状态\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_peft",
   "language": "python",
   "name": "new_peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
